# Standard library
import datetime
import io
import json
import logging
import pathlib
import re
import shutil
import sys
import zipfile
from pathlib import Path
from typing import Dict, List, Optional
from uuid import UUID, uuid4

# Third party
from celery import Celery
from pydantic import BaseModel, Field, constr

pathRoot = pathlib.Path(__file__).parents[1]
sys.path.append(str(pathRoot))

# Fairscape
from fairscape_mds.auth.ldap import getUserByCN
from fairscape_mds.config import get_fairscape_config
from fairscape_mds.models.dataset import DatasetDistribution, MinioDistribution
from fairscape_mds.models.evidencegraph import EvidenceGraph
from fairscape_mds.models.fairscape_base import IdentifierPattern
from fairscape_mds.models.rocrate import (
   ExtractCrate,
   DeleteExtractedCrate, 
   GetMetadataFromCrate,
   ROCrate,
   ROCrateDistribution,
   StreamZippedROCrate
)
from fairscape_mds.rocrate.errors import (
    ROCrateException
)
from fairscape_mds.models.user import UserLDAP
from fairscape_mds.utilities.operation_status import OperationStatus
from fairscape_mds.utilities.utils import parseArk

import boto3
from botocore.client import Config

# Configure logging
logging.getLogger('pymongo').setLevel(logging.INFO)

# setup logging
#logging.basicConfig(stream="", level=logging.INFO)
pathlib.Path('/tmp/jobs').mkdir(exist_ok=True)
fairscapeLogFolder = pathlib.Path("/tmp/logs")
fairscapeLogFolder.mkdir(exist_ok=True)

fairscapeWorkerLogfile = fairscapeLogFolder / 'worker.log'
workerLogHandler = logging.FileHandler(fairscapeWorkerLogfile)

backgroundTaskLogger = logging.getLogger("worker")
backgroundTaskLogger.addHandler(workerLogHandler)


# setup clients
fairscapeConfig = get_fairscape_config()
brokerURL = fairscapeConfig.redis.getBrokerURL()
    
minioClient = fairscapeConfig.minio.CreateClient()
mongoClient = fairscapeConfig.mongo.CreateClient()

mongoDB = mongoClient[fairscapeConfig.mongo.db]
asyncCollection = mongoDB[fairscapeConfig.mongo.async_collection]
identifierCollection = mongoDB[fairscapeConfig.mongo.identifier_collection]
rocrateCollection = mongoDB[fairscapeConfig.mongo.rocrate_collection]

s3 = boto3.client('s3',
    endpoint_url=fairscapeConfig.minio.host + ":" + fairscapeConfig.minio.port,
    aws_access_key_id=fairscapeConfig.minio.access_key,
    aws_secret_access_key=fairscapeConfig.minio.secret_key,
    config=Config(signature_version='s3v4'),
    region_name='us-east-1'
)

def _add_header(request, **kwargs):
	request.headers.add_header('x-minio-extract', 'true')

event_system = s3.meta.events
event_system.register_first('before-sign.s3.*', _add_header)

celeryApp = Celery()
celeryApp.conf.broker_url = brokerURL

celeryApp.conf.update(
    task_concurrency=4,  # Use 4 threads for concurrency
    worker_prefetch_multiplier=4  # Prefetch one task at a time
)

def serializeTimestamp(time):
    if time:
        return time.timestamp()
    else:
        return None

class ROCrateUploadJob(BaseModel):
    userCN: str
    transactionFolder: str
    zippedCratePath: str
    timeStarted: datetime.datetime | None = Field(default=None)
    timeFinished: datetime.datetime | None = Field(default=None)
    progress: float = Field(default=0)
    stage: Optional[str] = Field(default='started')
    status: Optional[str] = Field(default='in progress')
    completed: Optional[bool] = Field(default=False)
    success: Optional[bool] = Field(default=False)
    processedFiles: List[str] = Field(default=[])
    identifiersMinted: List[str] = Field(default=[])
    error: str | None = Field(default=None)


def createUploadJob(
        asyncJobCollection,
        userCN: str,
        transactionFolder: str, 
        zippedCratePath: str,
        ):
    ''' Insert a record into mongo for the submission of a job.

    Keyword arguments:
    transactionFolder -- (str) the UUID representing the unique path in minio
    zippedCratePath   -- (str) the filename of the zipped crate contents
    '''

    # setup job model
    uploadJobInstance = ROCrateUploadJob(
        userCN = userCN,
        transactionFolder=transactionFolder,
        zippedCratePath=zippedCratePath,
        timeStarted= datetime.datetime.now(tz=datetime.timezone.utc),
    )

    insertResult = asyncJobCollection.insert_one(
            uploadJobInstance.model_dump()
            )

    return uploadJobInstance 


def getUploadJob(
        asyncJobCollection,
        transactionFolder: str,
    ):
    ''' Return a upload Job record from mongo by the job UUID generated by celery.

    Keyword arguments:
    transactionFolder -- (str) the UUID representing the unique path in minio
    zippedCratePath   -- (str) the filename of the zipped crate contents
    '''

    jobMetadata = asyncJobCollection.find_one(
        {"transactionFolder": transactionFolder},
        { "_id": 0}
    )

    if jobMetadata:
        return ROCrateUploadJob.model_validate(jobMetadata)
    else:
        return None


def ProcessMetadata(roCrateMetadata, userCN: str, zipname: str):
    """ Function for processing metadata
    """ 
    crateArk = parseArk(roCrateMetadata["@id"])

    # Add distribution information if not present
    if 'distribution' not in roCrateMetadata:
        crate_name = Path(zipname).stem  # Get filename without extension
        object_path = f"{transactionFolder}/{crate_name}"

        roCrateMetadata['distribution'] = {
            "archivedROCrateBucket": fairscapeConfig.minio.rocrate_bucket,
            "archivedObjectPath": pathlib.Path(fairscapeConfig.minio.rocrate_bucket_path) / userCN / filePath / zipname
        }
        # set download link to https download link
        roCrateMetadata['contentURL'] = f"{fairscapeConfig.url}/rocrate/download/{rocrateGUID}" 

    roCrateMetadata['uploadDate'] = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    if crateArk is None:
        # TODO assign new identifiers
        pass
    else:
        roCrateMetadata["@id"] = crateArk

    # TODO reassign identifiers if there is conflict
    for crateElement in roCrateMetadata["@graph"]:
        elementArk = parseArk(crateElement["@id"])
        if elementArk is None:
            pass
        else:
            crateElement["@id"] = elementArk


def extractMetadata(transactionFolder, zippedMetadataKey):
    try:
        metadataResponse = s3.get_object(
            Bucket=fairscapeConfig.minio.default_bucket,
            Key=str(zippedMetadataKey),
        )

    # handle no such key error
    except s3.exceptions.NoSuchKey as err:
        backgroundTaskLogger.error(
            f"transaction: {str(transactionFolder)}" +
            "\tmessage: failed to read ro-crate-metadata.json from zipped ROCrate" + f"\terror: {str(minioException)}"
        )

        asyncCollection.update_one(
            {"transactionFolder": str(transactionFolder)},
            {"$set": 
                {
                    "completed": True,
                    "success": False,
                    "error": f"failed to read ro-crate-metadata.json from path {str(zippedMetadataKey)}",
                    "status": "Failed"
                }
            }
        )

    try:
        metadataContent = metadataResponse['Body'].read()
        metadataJSON = json.loads(metadataContent)

    # TODO more specific exceptions
    except:
        backgroundTaskLogger.error(
            f"transaction: {str(transactionFolder)}" +
            "\tmessage: failed to parse ro-crate-metadata.json from zipped ROCrate" + f"\terror: {str(minioException)}"
        )
        
        asyncCollection.update_one(
            {"transactionFolder": str(transactionFolder)},
            {"$set": 
                {
                    "completed": True,
                    "success": False,
                    "error": f"failed to parse ro-crate json",
                    "status": "Failed"
                }
            }
        )

        raise Exception

    asyncCollection.update_one(
        {"transactionFolder": str(transactionFolder)},
        {"$set": 
            {
                "stage": "extracting ro crate"
            }
        }
    )

    return metadataJSON


@celeryApp.task(name='async-register-ro-crate')
def AsyncRegisterROCrate(userCN: str, transactionFolder: str, filePath: str):
    """
    Background task for processing Zipped ROCrates.
    :param str userCN: Current User's CN uploading the ROCrate
    :param str transactionFolder: UUID folder representing the unique path in minio
    :param str filePath: The filename of the zipped crate contents
    """

    # connect to ldap and get user
    ldapConnection = fairscapeConfig.ldap.connectAdmin()
    currentUserLDAP = getUserByCN(ldapConnection, userCN)
    ldapConnection.unbind()


    # extract metadata from the zipped ROCrate 
    # construct the upload path
    uploadPath = pathlib.PurePosixPath(filePath) 
    
    zippedMetadataKey = uploadPath / uploadPath.stem / 'ro-crate-metadata.json'

    crateMetadata = extractMetadata(
        transactionFolder,
        zippedMetadataKey
    )

    # ------------------------------------
    #         Process Metadata 
    # -----------------------------------
    
    asyncCollection.update_one(
        {"transactionFolder": str(transactionFolder)},
        {"$set": 
            {
                "stage": "processing metadata"
            }
        }
    )
 
    
    # TODO reassign identifiers if there is conflict
    crateGUID = parseArk(crateMetadata["@id"])
    crateMetadata['@id'] = crateGUID

    # TODO add default project for a user

    # TODO if no ROCrate ARK is assigned 
    # if crateMetadata.get("@id") is None:
    #    pass

    # Add distribution information if not present
    if crateMetadata.get('distribution') is None:
        zipUploadPath = uploadPath 
        crateMetadata['distribution'] = {
            "archivedROCrateBucket": fairscapeConfig.minio.rocrate_bucket,
            "archivedObjectPath": str(zipUploadPath)
        }
        # set download link to https download link
        crateMetadata['contentUrl'] = f"{fairscapeConfig.url}/rocrate/download/{crateGUID}" 

    crateMetadata['uploadDate'] = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")


    for crateElement in crateMetadata["@graph"]:
        #if elementArk is None:
        #    pass
        elementArk = parseArk(crateElement["@id"])
        crateElement['@id'] = elementArk
        crateElement['isPartOf'] = {"@id": crateGUID}


    # upload extracted files to datasets 
    # filter out all datasets
    crateDatasets = filter(
        lambda crateElem: crateElem.get("@type") == "EVI:Dataset" and  crateElem.get("contentUrl") is not None,
        crateMetadata['@graph']
        )

    for datasetElem in crateDatasets:

        # file to read from within the zipfile 
        contentURL = datasetElem['contentUrl']
        sourcePath = pathlib.Path(contentURL.lstrip('file:///'))


        # compute relative crate path
        # this will give elements a path in minio preserving the structure of the rocrate
        # files will have relative path similar to
        #  {ro_crate_name} / *** / {filename}
        # starting from the deepest element breaking at the level of the rocrate
        folderNames= str(sourcePath).split('/')
        folderNames.reverse()

        relativePath = pathlib.Path(folderNames[0])

        for nested in folderNames[1::]:
            relativePath = nested / relativePath
            if nested == uploadPath.stem:
                break

        zippedFilePath = uploadPath / relativePath


        #backgroundTaskLogger.info(                
        #    f"transaction: {str(transactionFolder)}" +
        #    f"\tuploadPath: {str(uploadPath)}" +
        #    f"\ttmpFilePath: {str(tmpFilePath)}" +
        #    "\tmessage: uploading dataset" 
        #    )

        # add distribution to metadata to enable download enpoints
        datasetElem['distribution'] = {
            'distributionType': 'minio',
            'location': {
                'path': str(zippedFilePath)
            }
        }

        # TODO check that dataset enpoint works and is accurate
        # set download link relative to fairscape
        datasetElem['contentUrl']=f"{fairscapeConfig.url}/dataset/download/{datasetElem['@id']}"


    # --------------------------- 
    #    PUBLISH METADATA
    # --------------------------

    asyncCollection.update_one(
        {"transactionFolder": transactionFolder},
        {"$set": 
            {
                "stage": "publishing metadata"
            }
        }
    )

    # Check if @id already exsists
    #rocrateFound = rocrateCollection.find_one(
    #        {"@id": crateMetadata['@id']}
    #        )

    #if rocrateFound:
    #    raise ROCrateException(
    #        f"ROCrate with @id == {crateMetadata['@id']} found", None)
    

    # set default permissions for uploaded crate
    crateMetadata['permissions'] = {
            "owner": currentUserLDAP.dn,
            "group": currentUserLDAP.memberOf[0]
            }

    # set default permissions for all datasets
    for crateElem in crateMetadata['@graph']:
        # set permissions on all rocrate identifiers
        crateElem['permissions'] = {
            "owner": currentUserLDAP.dn,
            "group": currentUserLDAP.memberOf[0]
            }

    # for every element in the rocrate model dump json
    insertMetadata = [ elem for elem in crateMetadata.get("@graph", [])]
    # insert rocrate json into identifier collection
    insertMetadata.append(crateMetadata)

    insertedIdentifiers = [ elem.get("@id") for elem in insertMetadata]

    # TODO check for already existing identifiers and mark as conflicts
    #     - should reassign identifiers at metadata processing stage

    # insert all identifiers into the identifier collection
    insertResult = identifierCollection.insert_many(insertMetadata)

    if len(insertResult.inserted_ids) != len(insertMetadata):
        # raise an exception
        backgroundTaskLogger.error(
            f"transaction: {str(transactionFolder)}" +
            "\tmessage: error uploading provenance identifiers" 
        )
        raise Exception(f"Error Minting Provenance Identifiers")

    else:
        # log success
        backgroundTaskLogger.info(
            f"transaction: {str(transactionFolder)}" +
            "\tmessage: minted provenance identifiers" +
            f"\tcrateGUID: {crateMetadata['@id']}" +
            f"\tnumberOfIdentifiers: {len(insertResult.inserted_ids)}"
        )

    # insert rocrate result 
    insertResult = rocrateCollection.insert_one(crateMetadata)

    if insertResult.inserted_id is None:
        backgroundTaskLogger.error(
            f"transaction: {str(transactionFolder)}" +
            "\tmessage: error uploading rocrate identifier"  +
            f"\tcrateGUID: {crateMetadata['@id']}"
        )
        raise Exception(f"Error Minting Provenance Identifiers")

    else:
        backgroundTaskLogger.info(
            f"transaction: {str(transactionFolder)}" +
            "\tmessage: minted rocrate identifiers" +
            f"\tcrateGUID: {crateMetadata['@id']}" 
        )

    # update the job as success 
    # upsert because some jobs can't find their metadata
    asyncCollection.update_one(
        {"transactionFolder": str(transactionFolder)},
        {"$set": 
            {
                "userCN": userCN,
                "transactionFolder": str(transactionFolder),
                "zippedCratePath": filePath,
                "completed": True,
                "success": True,
                "status": "finished",
                "stage": "completed all tasks successfully"
            }
        },
        upsert=True
    )

    # ---------------------
    # Delete Transaction
    # ---------------------
    return True



def OldExtract():
    # extracting crate from path
    try:
        #roCrateMetadata = ExtractCrate(
        #    fairscapeConfig=fairscapeConfig,
        #    userCN=userCN,
        #    transactionFolder=transactionFolder,
        #    objectPath=filePath
        #)
        pass

        # update the uploadJob record
        #if roCrateMetadata is None:
        #    updateUploadJob(
        #        transactionFolder,
        #        {
        #            "completed": True,
        #            "success": False,
        #            "error": "error reading ro-crate-metadata",
        #            "status": "Failed"
        #        }
        #    )
        # return False
    except:
        updateUploadJob(
            transactionFolder,
            {
                "completed": True,
                "success": False,
                "error": "No ro-crate-metadata.json in zip file",
                "status": "Failed"
            }
        )
        return False


    # Process rocrate metadata
    updateUploadJob(
        transactionFolder,
        {"status": "minting identifiers"}
    )

    # TODO reason the rocrate metadata locally 
    # TODO reason over the rocrate metadata globally
    
    # TODO overwrite the rocrate metadata
    # overwriteZippedCrateMetadata(
    #    crateMetadata = rocrateMetadata,
    #    transactionFolder= transactionFolder,
    #)


    try:
        publishMetadata = PublishMetadata(
            currentUser=currentUser,
            rocrateJSON=roCrateMetadata,
            transactionFolder=transactionFolder,
            rocrateCollection=rocrateCollection,
            identifierCollection=identifierCollection,
        )
    except:
        updateUploadJob(
            transactionFolder,
            {
                "status": "Failed",
                "timeFinished": datetime.datetime.now(tz=datetime.timezone.utc),
                "completed": True,
                "success": False,
                "error": "Crate already exists on Fairscape."
            }
        )
        return False

    if publishMetadata is None:
        updateUploadJob(
            transactionFolder,
            {
                "status": "Failed",
                "timeFinished": datetime.datetime.now(tz=datetime.timezone.utc),
                "completed": True,
                "success": False,
            }
        )
        return False
    else:
        backgroundTaskLogger.info(
            f"transaction: {str(transactionFolder)}\t" +
            "message: task succeeded"
        )
        updateUploadJob(
            transactionFolder,
            {
                "status": "Finished",
                "timeFinished": datetime.datetime.now(tz=datetime.timezone.utc),
                "completed": True,
                "success": True,
                "identifiersMinted": publishMetadata
            }
        )
        return True

@celeryApp.task(name='async-build-evidence-graph', task_id=None)
def AsyncBuildEvidenceGraph(userCN: str, NAAN: str, postfix: str, task_id: str):
    # Get task id from current request
    mongoClient = fairscapeConfig.mongo.CreateClient()
    mongoDB = mongoClient[fairscapeConfig.mongo.db]
    identifierCollection = mongoDB[fairscapeConfig.mongo.identifier_collection]
    asyncCollection = mongoDB[fairscapeConfig.mongo.async_collection]
    
    node_id = f"ark:{NAAN}/{postfix}"
    evidence_graph_id = f"ark:{NAAN}/evidence-graph-{postfix}"
    
    # Initial status document with task_id as primary key
    status_doc = {
        "task_id": task_id,
        "node_id": node_id,
        "evidence_graph_id": evidence_graph_id,
        "owner": userCN,
        "status": "Building graph",
        "stage": "Started query",
        "created_at": datetime.datetime.utcnow(),
        "completed": False,
        "success": False
    }
    
    # Insert initial status
    print(f"Creating new task status: {status_doc}")
    asyncCollection.insert_one(status_doc)
    
    try:
        # Check for existing evidence graph
        existing_node = identifierCollection.find_one({"@id": node_id})
        if existing_node and "hasEvidenceGraph" in existing_node:
            existing_graph_id = existing_node["hasEvidenceGraph"]["@id"]
            print(f"Found existing evidence graph: {existing_graph_id}")
            
            # Create temporary EvidenceGraph object for deletion
            existing_graph = EvidenceGraph(
                **{
                    "@id": existing_graph_id,
                    "name": "Temporary Graph Object",
                    "description": "Temporary Graph Object for Deletion",
                    "owner": userCN
                }
            )
            
            # Delete existing graph
            print("Deleting existing evidence graph")
            delete_status = existing_graph.delete(identifierCollection)
            if not delete_status.success:
                print(f"Warning: Failed to delete existing graph: {delete_status.message}")
                # Update status to reflect warning
                asyncCollection.update_one(
                    {"task_id": task_id},
                    {"$set": {
                        "status": "Warning",
                        "warning": f"Failed to delete existing graph: {delete_status.message}",
                        "stage": "Deletion of existing graph failed",
                        "updated_at": datetime.datetime.utcnow()
                    }}
                )
        
        # Create new evidence graph
        evidence_graph = EvidenceGraph(
            **{
                "@id": evidence_graph_id,
                "name": f"Evidence Graph for {node_id}",
                "description": f"Evidence Graph for {node_id}",
                "owner": userCN
            }
        )
        
        # Build graph
        print(f"Building graph for node: {node_id}")
        evidence_graph.build_graph(node_id, identifierCollection)
        
        # Update status - Graph built
        print("Graph built successfully, updating status")
        asyncCollection.update_one(
            {"task_id": task_id},
            {"$set": {
                "status": "Graph built",
                "stage": "Storing graph",
                "updated_at": datetime.datetime.utcnow()
            }}
        )
        
        # Store graph
        print("Attempting to store graph")
        create_status = evidence_graph.create(identifierCollection)
        if not create_status.success:
            print(f"Failed to create graph: {create_status.message}")
            asyncCollection.update_one(
                {"task_id": task_id},
                {"$set": {
                    "status": "Failed",
                    "error": create_status.message,
                    "stage": "Graph creation failed",
                    "completed": True,
                    "success": False,
                    "updated_at": datetime.datetime.utcnow()
                }}
            )
            return False
            
        # Update original node reference
        print("Updating node reference")
        identifierCollection.update_one(
            {"@id": node_id},
            {"$set": {"hasEvidenceGraph": {"@id": evidence_graph.guid}}}
        )
        
        # Update final success status
        print("Updating final success status")
        asyncCollection.update_one(
            {"task_id": task_id},
            {"$set": {
                "status": "Completed",
                "stage": "Graph stored successfully",
                "completed": True,
                "success": True,
                "updated_at": datetime.datetime.utcnow()
            }}
        )
        return True
        
    except Exception as e:
        print(f"Error in task {task_id}: {str(e)}")
        asyncCollection.update_one(
            {"task_id": task_id},
            {"$set": {
                "status": "Failed",
                "error": str(e),
                "stage": "Exception occurred",
                "completed": True,
                "success": False,
                "updated_at": datetime.datetime.utcnow()
            }}
        )
        return False
        
    finally:
        mongoClient.close()


if __name__ == '__main__':
    args = ['worker', '--loglevel=INFO']

    # clear all transactions
    transactionTempFiles = pathlib.Path('/tmp/jobs/') 
    for jobFolder in transactionTempFiles.glob("*"):
        shutil.rmtree(jobFolder)

    celeryApp.worker_main(argv=args)

