from celery import Celery
import shutil
import pathlib
import zipfile
import json
import logging
import sys
import pathlib
import io
import datetime
import re
from pathlib import Path

logging.getLogger('pymongo').setLevel(logging.INFO)


# temporary fix for importing module problems
# TODO change background tasks to module
pathRoot = pathlib.Path(__file__).parents[1]
sys.path.append(str(pathRoot))

from pydantic import BaseModel, Field
from fairscape_mds.config import get_fairscape_config
from fairscape_mds.utilities.utils import parseArk
from fairscape_mds.models.user import UserLDAP
from fairscape_mds.models.dataset import DatasetDistribution, MinioDistribution
from fairscape_mds.models.rocrate import (
    ExtractCrate,
    DeleteExtractedCrate,
    GetMetadataFromCrate,
    StreamZippedROCrate,
    ROCrate,
    ROCrateDistribution
)
from fairscape_mds.rocrate.errors import (
    ROCrateException
)
from fairscape_mds.auth.ldap import getUserByCN

from typing import List, Dict, Optional
from uuid import UUID, uuid4


# setup logging
#logging.basicConfig(stream="", level=logging.INFO)
pathlib.Path('/tmp/jobs').mkdir(exist_ok=True)
fairscapeLogFolder = pathlib.Path("/tmp/logs")
fairscapeLogFolder.mkdir(exist_ok=True)

fairscapeWorkerLogfile = fairscapeLogFolder / 'worker.log'
workerLogHandler = logging.FileHandler(fairscapeWorkerLogfile)

backgroundTaskLogger = logging.getLogger("worker")
backgroundTaskLogger.addHandler(workerLogHandler)


# setup clients
fairscapeConfig = get_fairscape_config()
brokerURL = fairscapeConfig.redis.getBrokerURL()

celeryApp = Celery()
celeryApp.conf.broker_url = brokerURL

celeryApp.conf.update(
    task_concurrency=4,  # Use 4 threads for concurrency
    worker_prefetch_multiplier=4  # Prefetch one task at a time
)

def serializeTimestamp(time):
    if time:
        return time.timestamp()
    else:
        return None

class ROCrateUploadJob(BaseModel):
    userCN: str
    transactionFolder: str
    zippedCratePath: str
    timeStarted: datetime.datetime | None = Field(default=None)
    timeFinished: datetime.datetime | None = Field(default=None)
    progress: float = Field(default=0)
    stage: Optional[str] = Field(default='started')
    status: Optional[str] = Field(default='in progress')
    completed: Optional[bool] = Field(default=False)
    success: Optional[bool] = Field(default=False)
    processedFiles: List[str] = Field(default=[])
    identifiersMinted: List[str] = Field(default=[])
    error: str | None = Field(default=None)


def createUploadJob(
        asyncJobCollection,
        userCN: str,
        transactionFolder: str, 
        zippedCratePath: str,
        ):
    ''' Insert a record into mongo for the submission of a job.

    Keyword arguments:
    transactionFolder -- (str) the UUID representing the unique path in minio
    zippedCratePath   -- (str) the filename of the zipped crate contents
    '''

    # setup job model
    uploadJobInstance = ROCrateUploadJob(
        userCN = userCN,
        transactionFolder=transactionFolder,
        zippedCratePath=zippedCratePath,
        timeStarted= datetime.datetime.now(tz=datetime.timezone.utc),
    )

    insertResult = asyncJobCollection.insert_one(
            uploadJobInstance.model_dump()
            )

    return uploadJobInstance 


def getUploadJob(
        asyncJobCollection,
        transactionFolder: str,
    ):
    ''' Return a upload Job record from mongo by the job UUID generated by celery.

    Keyword arguments:
    transactionFolder -- (str) the UUID representing the unique path in minio
    zippedCratePath   -- (str) the filename of the zipped crate contents
    '''

    jobMetadata = asyncJobCollection.find_one(
        {"transactionFolder": transactionFolder},
        { "_id": 0}
    )

    if jobMetadata:
        return ROCrateUploadJob.model_validate(jobMetadata)
    else:
        return None


def ProcessMetadata(roCrateMetadata, userCN: str, zipname: str):
    """ Function for processing metadata
    """ 
    crateArk = parseArk(roCrateMetadata["@id"])

    # Add distribution information if not present
    if 'distribution' not in roCrateMetadata:
        crate_name = Path(zipname).stem  # Get filename without extension
        object_path = f"{transactionFolder}/{crate_name}"

        roCrateMetadata['distribution'] = {
            "archivedROCrateBucket": fairscapeConfig.minio.rocrate_bucket,
            "archivedObjectPath": pathlib.Path(fairscapeConfig.minio.rocrate_bucket_path) / userCN / filePath / zipname
        }
        # set download link to https download link
        roCrateMetadata['contentURL'] = f"{fairscapeConfig.url}/rocrate/download/{rocrateGUID}" 

    roCrateMetadata['uploadDate'] = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    if crateArk is None:
        # TODO assign new identifiers
        pass
    else:
        roCrateMetadata["@id"] = crateArk

    # TODO reassign identifiers if there is conflict
    for crateElement in roCrateMetadata["@graph"]:
        elementArk = parseArk(crateElement["@id"])
        if elementArk is None:
            pass
        else:
            crateElement["@id"] = elementArk



@celeryApp.task(name='async-register-ro-crate')
def AsyncRegisterROCrate(userCN: str, transactionFolder: str, filePath: str):
    """
    Background task for processing Zipped ROCrates.
    :param str userCN: Current User's CN uploading the ROCrate
    :param str transactionFolder: UUID folder representing the unique path in minio
    :param str filePath: The filename of the zipped crate contents
    """

    # connect to ldap and get user
    ldapConnection = fairscapeConfig.ldap.connectAdmin()
    currentUserLDAP = getUserByCN(ldapConnection, userCN)
    ldapConnection.unbind()

    minioClient = fairscapeConfig.minio.CreateClient()
    mongoClient = fairscapeConfig.mongo.CreateClient()

    mongoDB = mongoClient[fairscapeConfig.mongo.db]
    asyncCollection = mongoDB[fairscapeConfig.mongo.async_collection]
    identifierCollection = mongoDB[fairscapeConfig.mongo.identifier_collection]
    rocrateCollection = mongoDB[fairscapeConfig.mongo.rocrate_collection]

    # download crate from minio and store in a temporary file
    # write zip to temporary file and alter metadata
    jobDir = pathlib.Path(f"/tmp/jobs/{transactionFolder}")
    jobDir.mkdir(exist_ok=True)
    tmpZipFilepath = jobDir / pathlib.Path(filePath).name
    crateStem = pathlib.Path(filePath).stem

    backgroundTaskLogger.info(
        f"transaction: {str(transactionFolder)}" +
        "\tmessage: extracting rocrate" +
        f"\tcrateStem: {str(crateStem)}" +
        f"\tzipFilepath: {str(tmpZipFilepath)}"
    )

    try:
        objectResponse = minioClient.fget_object(
            bucket_name=fairscapeConfig.minio.rocrate_bucket, 
            object_name=filePath,
            file_path=str(tmpZipFilepath)
        )
    except Exception as minioException:
        backgroundTaskLogger.error(
            f"transaction: {str(transactionFolder)}" +
            "\tmessage: failed to read minio object" + f"\terror: {str(minioException)}"
        )

        asyncCollection.update_one(
            {"transactionFolder": str(transactionFolder)},
            {"$set": 
                {
                    "completed": True,
                    "success": False,
                    "error": f"Failed to read minio Object \terror: {str(minioException)}",
                    "status": "Failed"
                }
            }
        )

        return False
    #finally:
    #    objectResponse.close()
    #    objectResponse.release_conn()

    asyncCollection.update_one(
        {"transactionFolder": str(transactionFolder)},
        {"$set": 
            {
                "stage": "extracting ro crate"
            }
        }
    )

    jobDir = pathlib.Path(f"/tmp/jobs/{transactionFolder}")
    extractDir = jobDir / 'extracts'
    extractDir.mkdir(exist_ok=True)

    # try reading the tmp zip filepath
    with tmpZipFilepath.open("rb") as tmpZipFileObj:
        zipCrate = zipfile.ZipFile(tmpZipFileObj)
        zipCrate.extractall(path=extractDir)

        backgroundTaskLogger.info(
            f"transaction: {str(transactionFolder)}" +
            "\tmessage: extracted files"  
        )
    
    # find the ro-crate-metadata.json file and read it in
    metadataSearch = list(pathlib.Path(extractDir).rglob("*ro-crate-metadata.json"))
    if len(metadataSearch) != 1:
        raise Exception("ro-crate-metadata.json not found in crate")	

    crateMetadataPath = metadataSearch[0]

    with crateMetadataPath.open("r") as crateMetadataFileObj:
        crateMetadata = json.load(crateMetadataFileObj)

    # ------------------------------------
    #         Process Metadata 
    # -----------------------------------
    
    asyncCollection.update_one(
        {"transactionFolder": str(transactionFolder)},
        {"$set": 
            {
                "stage": "processing metadata"
            }
        }
    )
 
    
    # TODO reassign identifiers if there is conflict
    crateGUID = parseArk(crateMetadata["@id"])
    crateMetadata['@id'] = crateGUID

    # TODO add default project for a user

    # TODO if no ROCrate ARK is assigned 
    # if crateMetadata.get("@id") is None:
    #    pass

    # Add distribution information if not present
    if crateMetadata.get('distribution') is None:
        zipUploadPath = pathlib.Path(fairscapeConfig.minio.rocrate_bucket_path) / userCN / 'rocrates' / pathlib.Path(filePath).name
        crateMetadata['distribution'] = {
            "archivedROCrateBucket": fairscapeConfig.minio.rocrate_bucket,
            "archivedObjectPath": str(zipUploadPath)
        }
        # set download link to https download link
        crateMetadata['contentUrl'] = f"{fairscapeConfig.url}/rocrate/download/{crateGUID}" 

    crateMetadata['uploadDate'] = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")


    for crateElement in crateMetadata["@graph"]:
        #if elementArk is None:
        #    pass
        elementArk = parseArk(crateElement["@id"])
        crateElement['@id'] = elementArk
        crateElement['isPartOf'] = {"@id": crateGUID}


    # upload extracted files to datasets 
    # filter out all datasets
    crateDatasets = filter(
        lambda crateElem: crateElem.get("@type") == "EVI:Dataset" and  crateElem.get("contentUrl") is not None,
        crateMetadata['@graph']
        )

    for datasetElem in crateDatasets:

        # file to read from within the zipfile 
        contentURL = datasetElem['contentUrl']
        sourcePath = pathlib.Path(contentURL.lstrip('file:///'))



        # compute relative crate path
        # this will give elements a path in minio preserving the structure of the rocrate
        # files will have relative path similar to
        #  {ro_crate_name} / *** / {filename}
        # starting from the deepest element breaking at the level of the rocrate
        folderNames= str(sourcePath).split('/')
        folderNames.reverse()

        relativePath = pathlib.Path(folderNames[0])

        for nested in folderNames[1::]:
            relativePath = nested / relativePath
            if nested == crateStem:
                break

        
        uploadPath = pathlib.Path(fairscapeConfig.minio.default_bucket_path) / currentUserLDAP.cn / 'datasets' / relativePath
        tmpFilePath = pathlib.Path('/tmp/jobs/') / transactionFolder / 'extracts' / relativePath


        #backgroundTaskLogger.info(                
        #    f"transaction: {str(transactionFolder)}" +
        #    f"\tuploadPath: {str(uploadPath)}" +
        #    f"\ttmpFilePath: {str(tmpFilePath)}" +
        #    "\tmessage: uploading dataset" 
        #    )


        try:
            uploadResult = minioClient.fput_object(
                    bucket_name=fairscapeConfig.minio.default_bucket,
                    object_name=str(uploadPath),
                    file_path=str(tmpFilePath),
                    metadata={
                        "guid": datasetElem.get("@id"),
                        "owner": userCN
                        }
                    )
        except Exception as e:
            backgroundTaskLogger.error(                
            f"transaction: {str(transactionFolder)}" +
            f"\tuploadPath: {str(uploadPath)}" +
            f"\ttmpFilePath: {str(tmpFilePath)}" +
            "\tmessage: error uploading processed crate DATASETS" +
            f"\texception: {str(e)}"
            )

            asyncCollection.update_one(
                {"transactionFolder": transactionFolder},
                { 
                    "$set": {
                        "complete": True,
                        "success": False,
                        "errors": {
                            "message": "error uploading processed crate datasets",
                            "exception": str(e)
                        }
                    }
                }
            )

            return False


        # add distribution to metadata to enable download enpoints
        datasetElem['distribution'] = {
            'distributionType': 'minio',
            'location': {
                'path': str(uploadPath)
            }
        }

        # TODO check that dataset enpoint works and is accurate
        # set download link relative to fairscape
        datasetElem['contentUrl']=f"{fairscapeConfig.url}/dataset/download/{datasetElem['@id']}"
        
    # persist changes to extract dir 
    with crateMetadataPath.open("w") as crateMetadataFileObj:
        json.dump(crateMetadata, crateMetadataFileObj, indent=2)
    

    asyncCollection.update_one(
        {"transactionFolder": str(transactionFolder)},
        {"$set": 
            {
                "stage": "zipping rocrate with processed metadata"
            }
        }
    )

    # overwrite zipfile    
    with zipfile.ZipFile(tmpZipFilepath, mode="w") as zipCrate:
        
        crateName = pathlib.Path(filePath).name
        contentDir = extractDir / crateStem

        for crateElement in contentDir.rglob("*"):
            keyName = crateElement.relative_to(
                f'/tmp/jobs/{transactionFolder}/extracts/{crateStem}'
            )
            zipCrate.write(filename=crateElement, arcname=keyName)

    # write final rocrate into archive at the path   
    # / default / {userCN} / rocrates / crateName
    try:
        objectResponse = minioClient.fput_object(
            bucket_name=fairscapeConfig.minio.rocrate_bucket, 
            object_name=str(zipUploadPath),
            file_path=str(tmpZipFilepath)
        )
        
        backgroundTaskLogger.info(
            f"transaction: {str(transactionFolder)}" +
            "\tmessage: uploaded new zip" 
            )

    except Exception as e:
        backgroundTaskLogger.error(
            f"transaction: {str(transactionFolder)}" +
            "\tmessage: error uploading processed ROCrate Archive" +
            f"\texception: {str(e)}"
            )

    # --------------------------- 
    #    PUBLISH METADATA
    # --------------------------

    asyncCollection.update_one(
        {"transactionFolder": transactionFolder},
        {"$set": 
            {
                "stage": "publishing metadata"
            }
        }
    )

    # Check if @id already exsists
    rocrateFound = rocrateCollection.find_one(
            {"@id": crateMetadata['@id']}
            )

    if rocrateFound:
        raise ROCrateException(
            f"ROCrate with @id == {crateMetadata['@id']} found", None)
    

    # set default permissions for uploaded crate
    crateMetadata['permissions'] = {
            "owner": currentUserLDAP.dn,
            "group": currentUserLDAP.memberOf[0]
            }

    # set default permissions for all datasets
    for crateElem in crateMetadata['@graph']:
        # set permissions on all rocrate identifiers
        crateElem['permissions'] = {
            "owner": currentUserLDAP.dn,
            "group": currentUserLDAP.memberOf[0]
            }

    # for every element in the rocrate model dump json
    insertMetadata = [ elem for elem in crateMetadata.get("@graph", [])]
    # insert rocrate json into identifier collection
    insertMetadata.append(crateMetadata)

    insertedIdentifiers = [ elem.get("@id") for elem in insertMetadata]

    # TODO check for already existing identifiers and mark as conflicts
    #     - should reassign identifiers at metadata processing stage

    # insert all identifiers into the identifier collection
    insertResult = identifierCollection.insert_many(insertMetadata)

    if len(insertResult.inserted_ids) != len(insertMetadata):
        # raise an exception
        backgroundTaskLogger.error(
            f"transaction: {str(transactionFolder)}" +
            "\tmessage: error uploading provenance identifiers" 
        )
        raise Exception(f"Error Minting Provenance Identifiers")

    else:
        # log success
        backgroundTaskLogger.info(
            f"transaction: {str(transactionFolder)}" +
            "\tmessage: minted provenance identifiers" +
            f"\tcrateGUID: {crateMetadata['@id']}" +
            f"\tnumberOfIdentifiers: {len(insertResult.inserted_ids)}"
        )

    # insert rocrate result 
    insertResult = rocrateCollection.insert_one(crateMetadata)

    if insertResult.inserted_id is None:
        backgroundTaskLogger.error(
            f"transaction: {str(transactionFolder)}" +
            "\tmessage: error uploading rocrate identifier"  +
            f"\tcrateGUID: {crateMetadata['@id']}"
        )
        raise Exception(f"Error Minting Provenance Identifiers")

    else:
        backgroundTaskLogger.info(
            f"transaction: {str(transactionFolder)}" +
            "\tmessage: minted rocrate identifiers" +
            f"\tcrateGUID: {crateMetadata['@id']}" 
        )

    # update the job as success 
    # upsert because some jobs can't find their metadata
    asyncCollection.update_one(
        {"transactionFolder": str(transactionFolder)},
        {"$set": 
            {
                "userCN": userCN,
                "transactionFolder": str(transactionFolder),
                "zippedCratePath": filePath,
                "completed": True,
                "success": True,
                "status": "finished",
                "stage": "completed all tasks successfully"
            }
        },
        upsert=True
    )

    # close clients
    mongoClient.close()

    # ---------------------
    # Delete Transaction
    # ---------------------
    minioClient.remove_object(
        bucket_name=fairscapeConfig.minio.rocrate_bucket,
        object_name=filePath
    )

    # remove temp job files
    transactionTempFiles = pathlib.Path('/tmp/jobs/') / transactionFolder 

    shutil.rmtree(transactionTempFiles) 

    return True



def OldExtract():
    # extracting crate from path
    try:
        #roCrateMetadata = ExtractCrate(
        #    fairscapeConfig=fairscapeConfig,
        #    userCN=userCN,
        #    transactionFolder=transactionFolder,
        #    objectPath=filePath
        #)
        pass

        # update the uploadJob record
        #if roCrateMetadata is None:
        #    updateUploadJob(
        #        transactionFolder,
        #        {
        #            "completed": True,
        #            "success": False,
        #            "error": "error reading ro-crate-metadata",
        #            "status": "Failed"
        #        }
        #    )
        # return False
    except:
        updateUploadJob(
            transactionFolder,
            {
                "completed": True,
                "success": False,
                "error": "No ro-crate-metadata.json in zip file",
                "status": "Failed"
            }
        )
        return False


    # Process rocrate metadata
    updateUploadJob(
        transactionFolder,
        {"status": "minting identifiers"}
    )

    # TODO reason the rocrate metadata locally 
    # TODO reason over the rocrate metadata globally
    
    # TODO overwrite the rocrate metadata
    # overwriteZippedCrateMetadata(
    #    crateMetadata = rocrateMetadata,
    #    transactionFolder= transactionFolder,
    #)


    try:
        publishMetadata = PublishMetadata(
            currentUser=currentUser,
            rocrateJSON=roCrateMetadata,
            transactionFolder=transactionFolder,
            rocrateCollection=rocrateCollection,
            identifierCollection=identifierCollection,
        )
    except:
        updateUploadJob(
            transactionFolder,
            {
                "status": "Failed",
                "timeFinished": datetime.datetime.now(tz=datetime.timezone.utc),
                "completed": True,
                "success": False,
                "error": "Crate already exists on Fairscape."
            }
        )
        return False

    if publishMetadata is None:
        updateUploadJob(
            transactionFolder,
            {
                "status": "Failed",
                "timeFinished": datetime.datetime.now(tz=datetime.timezone.utc),
                "completed": True,
                "success": False,
            }
        )
        return False
    else:
        backgroundTaskLogger.info(
            f"transaction: {str(transactionFolder)}\t" +
            "message: task succeeded"
        )
        updateUploadJob(
            transactionFolder,
            {
                "status": "Finished",
                "timeFinished": datetime.datetime.now(tz=datetime.timezone.utc),
                "completed": True,
                "success": True,
                "identifiersMinted": publishMetadata
            }
        )
        return True




if __name__ == '__main__':
    args = ['worker', '--loglevel=INFO']

    # clear all transactions
    transactionTempFiles = pathlib.Path('/tmp/jobs/') 
    for jobFolder in transactionTempFiles.glob("*"):
        shutil.rmtree(jobFolder)

    celeryApp.worker_main(argv=args)

